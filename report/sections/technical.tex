This chapter presents the results of our research into the technical
underpinnings of the Kinect as a sensing device. Section \ref{howitworks}
describes techniques implemented in the Kinect to produces depth maps, in terms
of standard concepts from the field of machine vision. Then, section
\ref{precision} presents measurements, estimates and published data, which
together characterise the precision of the Kinect as a measurement tool.

\section{Overview: What is a Kinect}

The Kinect device is a sensor array incorporating, most notably, a system for
depth measurement. This system relies on a technique that enables low-cost
implementations. Several founders of an Israeli technology firm, PrimeSense Ltd,
invented the technique sometime before 11 October 2005 \cite{ZALEVSKY:2007}.
Microsoft corp., in turn, began selling the Kinect-branded implementation on 10
November 2010 as an optional user interface for its consumer gaming platform,
the Xbox 360.


\subsection{Specifications}

Table \ref{tab:specs} provides an overview of the electronic components that
comprise the Kinect's sensor array.

\begin{table}[ht]
\centering
\begin{tabular}{l p{10cm}}
\toprule
Component & Description \\
\midrule

RGB Camera & Sensor ``very similar'' to Mi\-cron mt9v112 (1/6" VGA CMOS) but
``larger'' and with some di\-ffering reg\-isters. Bay\-er co\-lour pa\-ttern (RG,GB).
640x\-480 pixels, 8-bit at 30 Hz.  1280x\-1024 pixels at 10\-Hz if using Open\-NI backend
, though this is referred to as ``15Hz'' in low level pro\-tocol. UYVY co\-lour
for\-mat also possible at ``15 Hz'' framerate.\cite{FREENECT}\cite{RGBDEMO} \\

Infrared camera & Monochrome CMOS sensor (no reliable source for this). Three
options for pixel size: ``small'' (unspecified); 640x480; 1280x1024. Framerate
options are 15Hz and 30Hz, except at maximum resolution (~9Hz).\cite{FREENECT}
field of view at 57 degrees horizontal, 43 degrees vertical according to retail
description.\cite{PLAY} Range reportedly ``adjustable'' (unconfirmed source) \\ 

Depth stream & Uncompressed data stream 16 bits, though this causes device
bandwidth problems. Usable options: ``differential/RLE'' compressed 11-bit
stream; 10-bit stream; 11-bit stream (uncompressed? still to be confirmed). The
only usable pixel size is 640x480. Framerate: 30Hz. \cite{FREENECT} Depth range
1.2m to 3.5m according to retail description.\cite{PLAY} Actual range
capability: ~ 0.7m-0.6m (unconfirmed source).\\

Infrared projector & Although non-reliable internet documents refe the use of a
``laser'' to project the infrared speckle pattern, this is unconfirmed. One
patent application relating to the Kinect describes a projecting a ``pattern of
coherent radiation''.\cite{SHPUNT:2010-1} Section \ref{howitworks} provides more
detail regading the projection component. \\

Accelerometer & Kionix KXSD9 Series. Sensitivity: 819 counts per g of
acceleration. Reports device tilt relative to the
horizon.\cite{FREENECT}\cite{KIONIX}\\

Microphone & ``Multiarray'' microphone consisting of four microphone units. Each
microphone generates two streams of 32 bit signed little endian PCM samples at
16KHz. A ninth channel from the device provides a unified noise-cancelled signal
in 16-bit little endian PCM samples (16KHz).\cite{FREENECT}\\

\bottomrule
\end{tabular}
\caption{Specifications of the Kinect}
\label{tab:specs}
\end{table}


\section{Theory: How it works}
\label{howitworks}

Our assessment is that the kinect measures depth using a proprietary extension
of the standard computer vision process known as stereo triangulation. In this
section we describe the depth measurement implementation in relatively high
level terms, from the perspective of theoretical computer vision. These
descriptions are based on our interpretation of public records, including
patents and scholarly as well as casual articles. We also refer, though to a
lesser extent, to our own experiments with the Kinect.


\subsection{Stereo triangulation}
\label{sub:triang}

Figure \ref{fig:triang} shows the idealised configuration of two cameras,
$c_{l}$ and $c_{r}$, set up as a stereo unit, and a point, $p$, on an object
viewed by both cameras. In this model, we simplify: 

\begin{itemize}

    \item   the sensors as pinhole cameras;

    \item   the stereo set-up so that the image planes of the two cameras
            (represented here for convenience' sake as positioned in front of
            the focal points) are coincident;

\end{itemize}

\begin{figure}[ht]
    \begin{center}
        \input{tikz_figures/triangulation}
        \caption{``Standard'' stereo triangulation}
        \label{fig:triang}
    \end{center}
\end{figure}

The desired value is $Z_{p}$, i.e.\ the minimum distance from the $x-y$ plane to
$p$. We obtain $Z_{p}$ from: 

\begin{itemize}

    \item the focal length ($f$);

    \item the inter-camera distance ($b$); and 

    \item the respective distances between left and right camera norms and the
    pixels corresponding to $p$ on the left and right image planes ($x_{l}$ and
    $x_{r}$),

\end{itemize}

using the similarity of triangles, which yields equation
\ref{eqn:triang1}.

%todo show algebra to derive this
\begin{align}
    \label{eqn:triang1}
    \frac{Z_p}{b} = \frac{Z_p - f}{b - x_l - x_r}
\end{align}


\subsection{Stereo triangulation: the Kinect way}

Figure \ref{fig:atriang} shows the triangulation model adapted, \emph{mutatis
mutandis}, to a depth camera ($c$) projector ($s$) pair. This is sometimes
referred to as ``active triangulation'' \cite{alexander1987}, since one replaces
one passive component (a camera) with an active one (a projector). The same
simplifications apply as in section \ref{sub:triang}. The projector replaces the
second camera here, and we distinguish the target point ($p_a$) on the object
plane from the corresponding point ($p_r$) on a predetermined reference plane.

\begin{figure}[ht]
    \begin{center}
        \input{tikz_figures/projection_triangulation}
        \caption{``Active'' stereo triangulation}
        \label{fig:atriang}
    \end{center}
\end{figure}

Now it is worth emphasising the order in which the Kinect proceeds: First, for
every point ($p_a$) in the depth image, it finds the matching point ($p_r$) in
the reference image by solving the so-called ``correspondence problem''
discussed in section \ref{sub:corr}. Once the $p_r$ in question is known, $p_a$
is determined as the intersection of the $(p_r,s)$ and $(c,x)$ lines, where $x$
is the target pixel on the image plane. Then it is again possible to derive the desired depth value, $Z_a$, from:

\begin{itemize}

    \item the focal length ($f$);

    \item the inter-camera distance ($b$);

    \item the distance between $x$ and the pixel corresponding to $p_r$ on the image plane ($d_i$);

    \item the distance between $p_a$ and the intersection of the $(c,p_r)$ point with the object plane ($d_o$); and

    \item the reference image depth ($Z_r$),

\end{itemize}

using the similarity of triangles, which yields the equation \ref{eqn:atriang1}.

%todo show algebra to derive this
\begin{align}
    \label{eqn:atriang1}
    Z_a = \frac{Z_r}{d \frac{Z_r}{(b f)} + 1}
\end{align}


\subsubsection{Experiment: pattern variation over distance}

%todo describe experiment with pictures... (first discuss whether useful with Jeroen)


\subsection{Correspondence problem}
\label{sub:corr}

%todo describe experiment with pictures... 


\section{Precision of the intrument}
\label{precision}

%todo: just take data from article
In this section we describe the Kinect's characteristics as a depth measurement
tool, and notably the precision of the measurements.


\subsection{Depth precision}

\subsubsection{Sources of error}

\subsection{Depth image resolution}

\subsubsection{Sources of error}
