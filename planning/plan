Project plan: Kinect 3-d object scanner
Date: 16 November 2011

Project team:
    Joris Stork, Lucas Swartsenburg, Sander van Veen, Bas Weelinck, Jimi van der
    Woning, Jeroen Zuiddam


Project description:

Our goal is threefold:

1. What is a Kinect? 
    To collect data describing the mechanisms by which the Kinect senses colour
    and depth data, and how it provides this to a host computer, via: online
    research; experiments; analysis of software drivers.

2. Precision of the Kinect as a scientific instrument.
    To collect data describing the Kinect's precision as a measurement tool, and
    to conduct experiments that may complement or confirm third those party
    findings.

3. Build a Kinect based application: 3-D object scanner
    To build a command-line interfaced 3-D "object scanning" application in
    Python, on top of ready-made (freenect) python wrappers for the Kinect. The
    application will involve taking multiple Kinect recordings (positioning the
    Kinect by hand for each recording), of a fixed object located at the ideal
    depth-measurement distance from the Kinect (~ 1.8m). A pattern of visible
    dots, squares or crosses is placed on the surface on which the object rests.
    Depth measurements are used to determine the plane incident with the visible
    pattern. For example, an algorithm could search for a large number of
    triangles connecting depth points, that align to the same plane. The
    configuration of the visible patterns (and their plane) is
    "known"/calibrated with respect to our 3D coordinate system. At least three
    dots must be visible in any recording, in order to determine the orientation
    of the object. The first recording is used to create an initial point cloud
    representing part of the surface of the object, and a coordinate system for
    the object is established by extracting the dot pattern from the rgb image.
    Then, for each subsequent recording: the dot pattern is again extracted from
    the rgb image, and a matrix to encode the transformation from the initial
    object coordinate system to the new coordinate system is derived (we
    conceive of it as "moving the object", not moving the camera). That matrix
    is used to transform the new point cloud from the depth recording. We
    overlay the resulting point cloud in the cube-space, thus extending our 3-D
    scan. Note that, because our implementation will be in Python, this process
    will work slowly, with a recording framerate in the order of seconds.


Time planning: NB: this might stretch into week 50.
    
    Calendar week 46: Sketch out application, assign tasks to subteams
        Sub-group A: --> theory (project parts 1 and 2): conduct first 
                         experiments.
        Sub-group B: --> application (project part 3): produce an application
                         functional model
        
    Calendar week 47: Develop modules, researct/write theory section
        Full draft of specifications and "how it works" sections of report
        should be completed by the end of this week.

    Calendar week 48: Develop modules
        Emphasis on integration and user interface.

    Calendar week 49: Make it presentable
        Sub-group 1: Finalise report
        Sub-group 2: Finalise and present slideshow presentation
        Sub-group 3: Tie up loose ends in code. 
                     Demo application.
